{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt          \n",
    "\n",
    "from numpy import linalg as LA\n",
    "from scipy import optimize\n",
    "from itertools import cycle\n",
    "from sklearn import preprocessing,svm, metrics\n",
    "from umap import UMAP\n",
    "from skimage import io\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.datasets import load_digits,load_iris\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.cluster import MiniBatchKMeans,MeanShift\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 1 - Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sign_mnist_train.csv\")\n",
    "df.head()\n",
    "df.shape\n",
    "letter2encode = {'A': 0,'B': 1,'C': 2,'D': 3,'E': 4,'F': 5,'G': 6,'H': 7,'I': 8,'K': 9,'L': 10,'M': 11,\n",
    "                'N': 12,'O': 13,'P': 14,'Q': 15,'R': 16,'S': 17,'T': 18,'U': 19,'V': 20,'W': 21,'X': 22, 'Y': 23}\n",
    "\n",
    "def fix_label_gap(l):\n",
    "    if(l>=9):\n",
    "        return (l-1)\n",
    "    else:\n",
    "        return l\n",
    "\n",
    "def encode(character):\n",
    "    return letter2encode[character]\n",
    "\n",
    "df['label'] = df['label'].apply(fix_label_gap)\n",
    "WORD = 'THANKS'\n",
    "word = np.array(list(WORD))\n",
    "embedded_word = list(map(encode, word))\n",
    "reduced_df = df[df['label'].isin(embedded_word)]\n",
    "reduced_df.shape\n",
    "X = reduced_df.loc[:, reduced_df.columns != 'label'].values\n",
    "y = reduced_df['label'].values\n",
    "plt.imshow(X[12].reshape(28,28))\n",
    "X_PCA = PCA(n_components=5).fit_transform(X)\n",
    "X_LDA = LDA(n_components=5).fit_transform(X,y)\n",
    "X_TSNE = TSNE().fit_transform(X)\n",
    "X_UMAP = UMAP(n_neighbors=15,\n",
    "                      min_dist=0.1,\n",
    "                      metric='correlation').fit_transform(X)\n",
    "\n",
    "fig = plt.figure(figsize=(50,40))\n",
    "plt.subplot(2,2,1)\n",
    "plt.scatter(X_PCA[:,0], X_PCA[:,1], c=y, cmap='Set1')\n",
    "plt.title(\"Principal Component Analysis\", fontsize=40)\n",
    "plt.subplot(2,2,2)\n",
    "plt.scatter(X_UMAP[:,0], X_UMAP[:,1], c=y, cmap='Set1')\n",
    "plt.title(\"Uniform Manifold Approximation and Projections\", fontsize=40)\n",
    "plt.subplot(2,2,3)\n",
    "plt.scatter(X_LDA[:,0], X_LDA[:,1], c=y, cmap='Set1')\n",
    "plt.title(\"Linear Discriminant Analysis\", fontsize=40)\n",
    "plt.subplot(2,2,4)\n",
    "plt.scatter(X_TSNE[:,0], X_TSNE[:,1], c=y, cmap='Set1')\n",
    "plt.title(\"t-Distributed Stochastic Neighbor Embedding\", fontsize=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Cuántos componentes se deben usar para explicar la variabilidad del 70% y 80% de los datos en PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Cargamos un dataset de 64 columnas\n",
    "digits = load_digits()\n",
    "\n",
    "#reducimos a 2 dimensiones el dataset\n",
    "pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "projected = pca.fit_transform(digits.data)\n",
    "\n",
    "#ploteamos el resultado\n",
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=digits.target, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('Spectral', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora observamos la varianza de los datos con la cantidad total de columnas del dataset (64 columnas)\n",
    "pca = PCA().fit(digits.data)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Numero de columnas')\n",
    "plt.ylabel('Varianza acumulada');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta curva cuantifica qué parte de la varianza total de 64 dimensiones está contenida dentro de las primeras columnas. Por ejemplo, vemos que con el dataset original, las primeras 10 columnas contienen aproximadamente el 75% de la varianza, mientras que se necesitan conservar alrededor de 50 columnas para describir cerca del 100% de la varianza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 2-Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_excel('Concrete_Data.xls')\n",
    "df = dataset.drop(['Concrete compressive strength(MPa, megapascals) '],axis=1)\n",
    "\n",
    "x = dataset.iloc[:, 0:7]\n",
    "y = dataset.iloc[:, 7]\n",
    "#Separo los datos de \"train\" en entrenamiento y prueba para probar los algoritmos\n",
    "X_train, X_test, y_train, y_test = train_test_split(x.values, y.values, test_size=0.2)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "adr = DecisionTreeRegressor(max_depth = 5)\n",
    "adr.fit(X_train, y_train)\n",
    "\n",
    "#Realizo una predicción\n",
    "Y_pred = adr.predict(X_test)\n",
    "from sklearn import tree\n",
    "from subprocess import check_call\n",
    "from IPython.display import Image as PImage\n",
    "\n",
    "# exportar el modelo a archivo .dot\n",
    "with open(r\"tree1.dot\", 'w') as f:\n",
    "     f = tree.export_graphviz(adr,\n",
    "                              out_file=f,\n",
    "                              max_depth = 7,\n",
    "                              impurity = True,\n",
    "                              feature_names = list(df)[:7],\n",
    "                              rounded = True,\n",
    "                              filled= True )\n",
    "        \n",
    "# Convertir el archivo .dot a png para poder visualizarlo\n",
    "check_call(['dot','-Tpng',r'tree1.dot','-o',r'tree1.png'])\n",
    "PImage(\"tree1.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficient of determination: %.2f'% metrics.r2_score(y_test, Y_pred))\n",
    "print('Mean squared error: %.2f' % metrics.mean_squared_error(y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 3-Clasificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, 2:]  # we only take the last two features.\n",
    "y = iris.target\n",
    "C = 1.0  # SVM regularization parameter\n",
    "h=.02\n",
    "# SVC with linear kernel\n",
    "svc = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
    "# LinearSVC (linear kernel)\n",
    "lin_svc = svm.LinearSVC(C=C).fit(X, y)\n",
    "# SVC with RBF kernel\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\n",
    "# SVC with polynomial (degree 3) kernel\n",
    "poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "np.arange(y_min, y_max, h))\n",
    "\n",
    "# title for the plots\n",
    "titles = ['SVC with linear kernel','LinearSVC (linear kernel)','SVC with RBF kernel','SVC with polynomial (degree 3) kernel']\n",
    "\n",
    "for i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    " \n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    " \n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    " \n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel('Petal length')\n",
    "    plt.ylabel('Petal width')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(titles[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es preferible optar por un kernel SVM lineal si se tiene una gran cantidad de atributos en los datos(> 1000) porque es más probable que los datos se puedan separar linealmente en un espacio de muchas dimensiones. tambien se puede usar RBF, pero siempre se debe realizar una validación cruzada de entre los parámetros  de configuracion para evitar un ajuste excesivo.\n",
    "\n",
    "En el caso del grafico de RBF se ve una mejor separacion de los datos debido a que aumenta el area de los puntos rojos y se reducen las areas tanto azul como celeste indicando que el algoritmo puede identificar mucho mejor a los grupos y a que grupo pertenece cada punto del dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calidad Vino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTILIZANDO UN SVC LINEAL\n",
    "df_vinos = pd.read_csv('calidad_vino.csv', sep=';')\n",
    "X = df_vinos.drop(['quality'], axis=1) #dejamos las features solamente\n",
    "y = df_vinos['quality'] #dejamos el target\n",
    "xs = X.values\n",
    "ys = y.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "svclassifier_vinos = svm.SVC(kernel='linear', gamma='scale', probability=True)  # Kernel lineal\n",
    "svclassifier_vinos.fit(X_train, y_train) \n",
    "y_pred = svclassifier_vinos.predict(X_test)\n",
    "pred_proba = svclassifier_vinos.predict_proba(X_test)\n",
    "\n",
    "#METRICAS\n",
    "print (\"f1 score macro\", metrics.f1_score(y_test, y_pred, average='macro'))\n",
    "print(\"log loss\", metrics.log_loss(y_test, pred_proba))\n",
    "print (\"accuracy\",metrics.accuracy_score(y_test, y_pred))\n",
    "print (\"precision score\",metrics.precision_score(y_test, y_pred,average='macro')) \n",
    "#print(\"AUC\",metrics.roc_auc_score(y_test, y_pred)) #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTILIZANDO UN SVC con nucleo RBF\n",
    "df_vinos = pd.read_csv('calidad_vino.csv', sep=';')\n",
    "X = df_vinos.drop(['quality'], axis=1) #dejamos las features solamente\n",
    "y = df_vinos['quality'] #dejamos el target\n",
    "xs = X.values\n",
    "ys = y.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "svclassifier_vinos = svm.SVC(kernel='rbf', gamma='scale', probability=True)  # Kernel lineal\n",
    "svclassifier_vinos.fit(X_train, y_train) \n",
    "y_pred = svclassifier_vinos.predict(X_test)\n",
    "pred_proba = svclassifier_vinos.predict_proba(X_test)\n",
    "\n",
    "#METRICAS\n",
    "print (\"f1 score macro\", metrics.f1_score(y_test, y_pred, average='macro'))\n",
    "print(\"log loss\", metrics.log_loss(y_test, pred_proba))\n",
    "print (\"accuracy\",metrics.accuracy_score(y_test, y_pred))\n",
    "print (\"precision score\",metrics.precision_score(y_test, y_pred,average='macro')) \n",
    "#print(\"AUC\",metrics.roc_auc_score(y_test, y_pred)) #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores promedio para todas las columnas cuando se busca la calidad 6 en los vinos\n",
    "df_vinos[df_vinos.quality == 6].describe().iloc[[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area de decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Support Vector Machine Classifier\n",
    "df_vinos = pd.read_csv('calidad_vino.csv', sep=';')\n",
    "#X = df_vinos.drop(['quality'], axis=1) #dejamos las features solamente\n",
    "X = df_vinos.drop(df_vinos.columns[[2,3,4,5,6,7,8,9,10,11]], axis=1) #dejamos solo los dos atributos de acidez\n",
    "y = df_vinos['quality'] #dejamos el target\n",
    "\n",
    "clf = svm.SVC(kernel='linear',decision_function_shape='ovo')\n",
    "clf.fit(X.values, y.values) \n",
    "\n",
    "# Plot Decision Region using mlxtend's awesome plotting function\n",
    "plot_decision_regions(X=X.values, y=y.values, clf=clf, legend=2)\n",
    "\n",
    "# Update plot object with X/Y axis labels and Figure Title\n",
    "plt.xlabel(X.columns[0], size=14)\n",
    "plt.ylabel(X.columns[1], size=14)\n",
    "plt.title('SVM Decision Region Boundary', size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 4 - Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pixels(data, title, colors=None, N=10000):\n",
    "    if colors is None:\n",
    "        colors = data\n",
    "    \n",
    "    # choose a random subset\n",
    "    rng = np.random.RandomState(0)\n",
    "    i = rng.permutation(data.shape[0])[:N]\n",
    "    colors = colors[i]\n",
    "    R, G, B = data[i].T\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    ax[0].scatter(R, G, color=colors, marker='.')\n",
    "    ax[0].set(xlabel='Red', ylabel='Green', xlim=(0, 1), ylim=(0, 1))\n",
    "\n",
    "    ax[1].scatter(R, B, color=colors, marker='.')\n",
    "    ax[1].set(xlabel='Red', ylabel='Blue', xlim=(0, 1), ylim=(0, 1))\n",
    "\n",
    "    fig.suptitle(title, size=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foto = io.imread('colores.jpg') #TODO agregar un control para jupyter\n",
    "data = foto / 255.0\n",
    "data = data.reshape(foto.shape[0] * foto.shape[1], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = MiniBatchKMeans(80)\n",
    "kmeans.fit(data)\n",
    "new_colors = kmeans.cluster_centers_[kmeans.predict(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foto_recolored = new_colors.reshape(foto.shape)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6), subplot_kw=dict(xticks=[], yticks=[]))\n",
    "fig.subplots_adjust(wspace=0.05)\n",
    "ax[0].imshow(foto)\n",
    "ax[0].set_title('Original Image', size=16)\n",
    "ax[1].imshow(foto_recolored)\n",
    "ax[1].set_title('80-color Image', size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este algoritmo se puede utilizar para reducir el tamaño de imagenes y puede utilizarse para cargar imagenes cuando la conexion es de baja velocidad o para hacer gifs o para imagenes del tipo thumbnail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pixels(data, title='Espacio de colores de la imagen original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pixels(data, colors=new_colors,title=\"Reduced color space: 80 colors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_iris()\n",
    "ms = MeanShift()\n",
    "ms.fit(dataset.data)\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "n_clusters_ = labels.max()+1\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "colors = cycle('rcmy')\n",
    "\n",
    "for k, col in zip(range(n_clusters_), colors):\n",
    "    my_members = labels == k\n",
    "    cluster_center = cluster_centers[k]\n",
    "    plt.plot(dataset.data[my_members, 0], dataset.data[my_members, 1], col + '.')\n",
    "    plt.plot(cluster_center[0], cluster_center[1],\n",
    "             'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=14)\n",
    "plt.title('Cant. de clusters : %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 5 - Métodos de Ensambles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USANDO RANDOM FOREST COMO METODO DE CLASIFICACION DE ENSAMBLE\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# create the classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Train the model using the training sets\n",
    "classifier.fit(X_train, y_train)\n",
    "# predictin on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "# Calculate Model Accuracy\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=200)\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, predictions))\n",
    "print(metrics.confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple vista el metodo de ensamble que utiliza el algoritmo **RandomForestClassifier** tiene un mayor nivel de\n",
    "confiabilidad con respecto al algoritmo **AdaBoostClassifier** siendo que ambos trabajan sobre el mismo dataset.\n",
    "Ademas se observa que la suma de los elementos de la diagonal de la matriz de confusion del algoritmo **RandomForestClassifier** da un mayor valor que la del segundo algoritmo indicando que pudo identificar mejor a los elementos de las clases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
